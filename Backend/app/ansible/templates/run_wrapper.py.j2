#!/usr/bin/env python3
"""
Model Deployment Wrapper Script.

This script reads the deployment configuration and launches the appropriate
model serving framework (vLLM, SGLang, or Xinference).

Generated by Noveris AI Platform - DO NOT EDIT MANUALLY.

Deployment ID: {{ deployment_id }}
Framework: {{ framework }}
Model: {{ model_repo_id }}
"""

import json
import os
import subprocess
import sys
from pathlib import Path


def load_config() -> dict:
    """Load deployment configuration."""
    config_path = Path(__file__).parent / "config.json"
    with open(config_path) as f:
        return json.load(f)


def build_vllm_command(config: dict) -> list:
    """
    Build vLLM serve command.

    Reference: https://docs.vllm.ai/en/latest/getting_started/quickstart/
    """
    cmd = [
        "vllm", "serve",
        config["model_path"],
        "--host", config["host"],
        "--port", str(config["port"]),
    ]

    # Add served model name
    if config.get("served_model_name"):
        cmd.extend(["--served-model-name", config["served_model_name"]])

    # Add tensor parallel size
    if config.get("tensor_parallel_size", 1) > 1:
        cmd.extend(["--tensor-parallel-size", str(config["tensor_parallel_size"])])

    # Add GPU memory utilization
    if config.get("gpu_memory_utilization"):
        cmd.extend(["--gpu-memory-utilization", str(config["gpu_memory_utilization"])])

    # Add custom arguments
    for arg in config.get("args", []):
        if arg.get("enabled", True):
            key = arg["key"]
            value = arg.get("value", "")
            arg_type = arg.get("arg_type", "string")

            if arg_type == "bool":
                # Boolean flags are just the key
                if value.lower() in ("true", "1", "yes", ""):
                    cmd.append(key)
            elif value:
                cmd.extend([key, str(value)])
            else:
                cmd.append(key)

    return cmd


def build_sglang_command(config: dict) -> list:
    """
    Build SGLang launch_server command.

    Reference: https://docs.sglang.io/
    """
    cmd = [
        sys.executable, "-m", "sglang.launch_server",
        "--model-path", config["model_path"],
        "--host", config["host"],
        "--port", str(config["port"]),
    ]

    # Add tensor parallel size
    if config.get("tensor_parallel_size", 1) > 1:
        cmd.extend(["--tp", str(config["tensor_parallel_size"])])

    # Add GPU memory utilization (mem-fraction-static)
    if config.get("gpu_memory_utilization"):
        cmd.extend(["--mem-fraction-static", str(config["gpu_memory_utilization"])])

    # Add custom arguments
    for arg in config.get("args", []):
        if arg.get("enabled", True):
            key = arg["key"]
            value = arg.get("value", "")
            arg_type = arg.get("arg_type", "string")

            if arg_type == "bool":
                if value.lower() in ("true", "1", "yes", ""):
                    cmd.append(key)
            elif value:
                cmd.extend([key, str(value)])
            else:
                cmd.append(key)

    return cmd


def build_xinference_command(config: dict) -> list:
    """
    Build Xinference local command.

    Reference: https://inference.readthedocs.io/en/latest/getting_started/using_xinference.html
    """
    # Xinference uses a local server that models are launched into
    cmd = [
        "xinference-local",
        "--host", config["host"],
        "--port", str(config["port"]),
    ]

    return cmd


def build_environment(config: dict) -> dict:
    """Build environment variables for the subprocess."""
    env = os.environ.copy()

    # Add configured environment variables
    for env_item in config.get("env", []):
        name = env_item["name"]
        value = env_item.get("value", "")
        if value:
            env[name] = value

    return env


def main():
    """Main entry point."""
    # Load configuration
    config = load_config()
    framework = config["framework"]

    print(f"Starting {framework} server for model: {config['model_path']}")
    print(f"Listening on: {config['host']}:{config['port']}")

    # Build command based on framework
    if framework == "vllm":
        cmd = build_vllm_command(config)
    elif framework == "sglang":
        cmd = build_sglang_command(config)
    elif framework == "xinference":
        cmd = build_xinference_command(config)
    else:
        print(f"Unknown framework: {framework}", file=sys.stderr)
        sys.exit(1)

    # Build environment
    env = build_environment(config)

    print(f"Command: {' '.join(cmd)}")

    # Execute the command
    # Use exec to replace this process (for proper signal handling)
    try:
        os.execvpe(cmd[0], cmd, env)
    except FileNotFoundError:
        # If direct exec fails, try subprocess
        result = subprocess.run(cmd, env=env)
        sys.exit(result.returncode)


if __name__ == "__main__":
    main()
